{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjBV_8TaVdzF"
      },
      "source": [
        "# Named Entity Recognition with Zero-Shot LLMs: Project Introduction\n",
        "\n",
        "**Students:** Tiboni Gabriele, Visentin Giacomo  \n",
        "\n",
        "**Students ID:** 2102414, 2121345\n",
        "\n",
        "**Master Program:** Computer Engineering (AI & Robotics)\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Named-Entity Recognition (NER) is the task of identifying and classifying entities (persons, organizations, locations, etc.) in text. Modern instruction-tuned Large Language Models (LLMs) such as ChatGPT and Gemini can perform NER in zero-shot settings—without task-specific training.\n",
        "\n",
        "This project evaluates zero-shot NER with LLMs, based on [\"Empirical Study of Zero-Shot NER with ChatGPT\" (Xie et al., 2023)](https://arxiv.org/abs/2310.10035). We reproduce the baseline and two additional methods, test them on a standard dataset, and discuss results.\n",
        "\n",
        "---\n",
        "\n",
        "## Domain and Dataset\n",
        "\n",
        "We use the **CoNLL-2003** dataset: newswire text annotated with four entity types (Person, Location, Organization, Miscellaneous). The data is split into train, development, and test sets for fair evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l76fCsLWs2W"
      },
      "source": [
        "We start by installing and importing the main libraries for this project: Google Generative AI (for Gemini API), pandas, and the HuggingFace datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPYW5sxAGqhz",
        "outputId": "0b910373-023b-4c51-8f30-55b0ec6b9c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q google-generativeai datasets pandas\n",
        "!pip install --upgrade datasets fsspec\n",
        "\n",
        "# Import libraries\n",
        "import google.generativeai as genai  # For Gemini API\n",
        "import pandas as pd                  # For dataframes\n",
        "import time                          # To add delay between API calls\n",
        "import re                            # For regex parsing\n",
        "from datasets import load_dataset    # To load standard NER datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5uWHbq0XARG"
      },
      "source": [
        "Here we configure the Gemini API by setting the API key and initializing the model we will use for zero-shot NER.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdQ6yBQ-HG-x",
        "outputId": "4df6ddde-a20a-4818-c619-22ccc7375538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini configuration completed\n"
          ]
        }
      ],
      "source": [
        "# Set up Gemini API key and model, this is our API key, you can insert yours\n",
        "MY_API_KEY = \"INSERT HERE YOUR API KEY\"\n",
        "\n",
        "genai.configure(api_key=MY_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-2.5-flash-preview-05-20')\n",
        "\n",
        "print(\"Gemini configuration completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JymYAe-NXNEW"
      },
      "source": [
        "Now we load the CoNLL-2003 dataset from HuggingFace and check the size of each split. We also print the possible NER and POS tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA4zPDWYHPEZ",
        "outputId": "d5114d34-f769-443d-abdf-c3bcb1298566"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CoNLL-2003 dataset loaded\n",
            "Train: 14041 sentences\n",
            "Validation: 3250 sentences\n",
            "Test: 3453 sentences\n",
            "NER labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
            "POS labels: ['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"conll2003\")\n",
        "train_data = dataset[\"train\"]\n",
        "val_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "print(\"CoNLL-2003 dataset loaded\")\n",
        "print(f\"Train: {len(train_data)} sentences\")\n",
        "print(f\"Validation: {len(val_data)} sentences\")\n",
        "print(f\"Test: {len(test_data)} sentences\")\n",
        "\n",
        "# Get list of NER and POS tags\n",
        "ner_label_names = train_data.features['ner_tags'].feature.names\n",
        "pos_label_names = train_data.features['pos_tags'].feature.names\n",
        "print(f\"NER labels: {ner_label_names}\")\n",
        "print(f\"POS labels: {pos_label_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Eq1hj0XXub"
      },
      "source": [
        "We display the test set object to check its structure and contents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WjEgIozHdmH",
        "outputId": "1dd71af5-9be2-4de2-b663-5d075980ff2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "    num_rows: 3453\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the test dataset structure\n",
        "test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmbGUtGRXgo9"
      },
      "source": [
        "Here we define a function to extract sentences, NER labels, and POS tags from the HuggingFace dataset, limiting the number of examples for faster experiments. Then we prepare development and test subsets and print an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFlPpvKPHiJR",
        "outputId": "7aa15e30-bb11-439b-f9d0-dd57c8aebdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Development set: 30 sentences\n",
            "Test set: 50 sentences\n",
            "\n",
            "Example:\n",
            "Tokens: LONDON 1996-08-30\n",
            "Labels: B-LOC O\n"
          ]
        }
      ],
      "source": [
        "def prepare_data(hf_dataset, max_sentences=50):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    posTags = []\n",
        "\n",
        "    for i, example in enumerate(hf_dataset):\n",
        "        if i >= max_sentences:\n",
        "            break\n",
        "\n",
        "        tokens = example['tokens']\n",
        "        ner_tags = [ner_label_names[tag_id] for tag_id in example['ner_tags']]\n",
        "        pos_tags = [pos_label_names[tag_id] for tag_id in example['pos_tags']]\n",
        "\n",
        "        sentences.append(tokens)\n",
        "        labels.append(ner_tags)\n",
        "        posTags.append(pos_tags)\n",
        "\n",
        "    return sentences, labels, posTags\n",
        "\n",
        "# Prepare development and test sets (we use limited size for quick testing and for the usage limitation of our API key)\n",
        "dev_sentences, dev_labels, dev_pos = prepare_data(val_data, 30)\n",
        "test_sentences, test_labels, test_pos = prepare_data(test_data, 50)\n",
        "\n",
        "print(f\"Development set: {len(dev_sentences)} sentences\")\n",
        "print(f\"Test set: {len(test_sentences)} sentences\")\n",
        "\n",
        "# Show example sentence and labels\n",
        "print(f\"\\nExample:\")\n",
        "print(f\"Tokens: {' '.join(dev_sentences[1][:10])}\")\n",
        "print(f\"Labels: {' '.join(dev_labels[1][:10])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXBBujuiXvK-"
      },
      "source": [
        "The original CoNLL-2003 dataset uses detailed BIO tags (e.g., B-PER, I-LOC) to indicate the position of tokens within named entities. However, for our project, we simplify the tagging scheme to the IO format: each token is labeled with its entity type (e.g., PERSON, LOCATION) if it belongs to an entity, or 'O' if it does not.\n",
        "This choice keeps the labels simple and is suitable for testing if our zero-shot approach can find the right entities.\n",
        "The aim of this project is to explore how changing the prompt can improve NER results so using a simple IO tagging scheme is still enough to compare different methods and see which one works better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCueGwgSHuxW",
        "outputId": "550135b6-d635-49ef-ac5f-ac49069528de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels converted to simple format\n"
          ]
        }
      ],
      "source": [
        "# Function to convert CoNLL detailed labels to simple entity classes\n",
        "def convert_labels(conll_labels):\n",
        "    simple_labels = []\n",
        "\n",
        "    for label in conll_labels:\n",
        "        if label == 'O':\n",
        "            simple_labels.append('O')\n",
        "        elif 'PER' in label:\n",
        "            simple_labels.append('PERSON')\n",
        "        elif 'LOC' in label:\n",
        "            simple_labels.append('LOCATION')\n",
        "        elif 'ORG' in label:\n",
        "            simple_labels.append('ORGANIZATION')\n",
        "        elif 'MISC' in label:\n",
        "            simple_labels.append('MISCELLANEOUS')\n",
        "        else:\n",
        "            simple_labels.append('O')\n",
        "\n",
        "    return simple_labels\n",
        "\n",
        "# Convert dev and test labels to the simple format\n",
        "dev_labels_simple = [convert_labels(labels) for labels in dev_labels]\n",
        "test_labels_simple = [convert_labels(labels) for labels in test_labels]\n",
        "\n",
        "print(\"Labels converted to simple format\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_3XXJIMX3A-"
      },
      "source": [
        "We define the set of possible entity labels used for classification and print them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c995aP-DH98E",
        "outputId": "83314e94-f6f9-44a6-c81e-2e761a6dc8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available labels: {'LOCATION', 'PERSON', 'MISCELLANEOUS', 'O', 'ORGANIZATION'}\n"
          ]
        }
      ],
      "source": [
        "# Define the set of possible NER labels\n",
        "label_set = {'O', 'PERSON', 'LOCATION', 'ORGANIZATION', 'MISCELLANEOUS'}\n",
        "print(f\"Available labels: {label_set}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSvLQpxtYst2"
      },
      "source": [
        "We define four different zero-shot NER prompting methods for the LLM.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2IfU-mD2OV-"
      },
      "source": [
        "The first methos is the baseline method from Xie et al. (2023), which asks the model to classify each token directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bVYcW5X2D_x"
      },
      "outputs": [],
      "source": [
        "# METHOD 1: Baseline (from Xie et al., 2023)\n",
        "def get_baseline_prompt(sentence):\n",
        "    # Basic prompt, asks for token-level NER classification\n",
        "    text = \" \".join(sentence)\n",
        "    labels_str = \", \".join([f\"'{label}'\" for label in sorted(label_set)])\n",
        "\n",
        "    prompt = f\"\"\"Given entity label set: {{{labels_str}}}\n",
        "Based on the given entity label set, please recognize the named entities in the given text.\n",
        "Text: {text}\n",
        "\n",
        "Please classify each word in the text with the appropriate entity label. Use 'O' for words that are not named entities.\n",
        "Format your answer as: word1/LABEL1 word2/LABEL2 word3/O ...\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAcL3BZu2kjy"
      },
      "source": [
        "The second method adopts an approach similar to the decomposed QA strategy described in the paper (method b). The key difference is that, instead of making a separate API call for each label, we request all classifications within a single prompt. This choice was primarily due to API usage limitations. Nonetheless, it's particularly interesting to observe how the LLM performs the classification task after being explicitly instructed on the reasoning process to follow and how to combine the individual classifications into a final decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeOCHXyY2HV8"
      },
      "outputs": [],
      "source": [
        "# METHOD 2: Decomposed QA-style prompt\n",
        "def get_decomposed_qa_prompt(sentence):\n",
        "    # Asks for entities by type first, then for the full classification (all in one API call)\n",
        "    text = \" \".join(sentence)\n",
        "    labels_str = \", \".join([f\"'{label}'\" for label in sorted(label_set)])\n",
        "\n",
        "    prompt = f\"\"\"Given entity label set: {{{labels_str}}}\n",
        "Based on the given entity label set, please recognize the named entities in the given text.\n",
        "Text: {text}\n",
        "\n",
        "Question: What are the named entities labeled as 'PERSON' in the text?\n",
        "Answer: [List all PERSON entities found]\n",
        "\n",
        "Question: What are the named entities labeled as 'LOCATION' in the text?\n",
        "Answer: [List all LOCATION entities found]\n",
        "\n",
        "Question: What are the named entities labeled as 'ORGANIZATION' in the text?\n",
        "Answer: [List all ORGANIZATION entities found]\n",
        "\n",
        "Question: What are the named entities labeled as 'MISCELLANEOUS' in the text?\n",
        "Answer: [List all MISCELLANEOUS entities found]\n",
        "\n",
        "Now, based on the above analysis, classify each word:\n",
        "Format: word1/LABEL1 word2/LABEL2 word3/O ...\n",
        "\n",
        "Final Answer:\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huiE9OvF2kGq"
      },
      "source": [
        "The third method is very similar to the previous one and is also inspired by one of the approaches in the paper, in this case, method (d). However, in this version, the prompt includes not only the sentence but also its POS tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw91grPhIBbj"
      },
      "outputs": [],
      "source": [
        "# METHOD 3: Tool Augmentation (POS tags included)\n",
        "def get_tool_augmentation_prompt(sentence, pos_tags):\n",
        "    # Adds POS tag info to the prompt, (but only one API call)\n",
        "    text = \" \".join(sentence)\n",
        "    labels_str = \", \".join([f\"'{label}'\" for label in sorted(label_set)])\n",
        "    pos_text = \" \".join([f\"{word}/{tag}\" for word, tag in zip(sentence, pos_tags)])\n",
        "\n",
        "    prompt = f\"\"\"Given entity label set: {{{labels_str}}}\n",
        "Given the text and the corresponding Part-of-Speech tags, please recognize the named entities in the given text.\n",
        "Text: {text}\n",
        "Part-of-Speech tags: {pos_text}\n",
        "\n",
        "Question: What are the named entities labeled as 'PERSON' in the text?\n",
        "Answer: [Analyze using POS tags to identify PERSON entities]\n",
        "\n",
        "Question: What are the named entities labeled as 'LOCATION' in the text?\n",
        "Answer: [Analyze using POS tags to identify LOCATION entities]\n",
        "\n",
        "Question: What are the named entities labeled as 'ORGANIZATION' in the text?\n",
        "Answer: [Analyze using POS tags to identify ORGANIZATION entities]\n",
        "\n",
        "Question: What are the named entities labeled as 'MISCELLANEOUS' in the text?\n",
        "Answer: [Analyze using POS tags to identify MISCELLANEOUS entities]\n",
        "\n",
        "Based on the above analysis and POS tags, classify each word:\n",
        "Format: word1/LABEL1 word2/LABEL2 word3/O ...\n",
        "\n",
        "Final Classification:\"\"\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrLtB1HC5XXo"
      },
      "source": [
        "Method 4 is a simple method we thought of, and it gave good results in previous runs. It's the same as the baseline in that it asks the model to classify the whole sentence without explaining the reasoning. However, it adds a short description of the labels or what each label includes. The idea is that by explaining what each label represents, the model can better understand which one to choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNPm8mrm2LhC"
      },
      "outputs": [],
      "source": [
        "# METHOD 4: Detailed definitions (extra method)\n",
        "def get_detailed_prompt(sentence):\n",
        "    # Extra method, uses explicit definitions for each class (not from the paper, just a reasonable variant)\n",
        "    text = \" \".join(sentence)\n",
        "    prompt = f\"\"\"Identify and classify each word in the following text as one of the following categories:\n",
        "\n",
        "PERSON: Names of people (first names, last names, nicknames).\n",
        "LOCATION: Geographic locations (cities, countries, continents, landmarks).\n",
        "ORGANIZATION: Companies, institutions, government agencies, teams, universities.\n",
        "MISCELLANEOUS: Languages, nationalities, religions, events, awards, products, brands.\n",
        "O: Not a named entity.\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Provide the classification for each word in the format: word/CATEGORY\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD5g4XsTZACy"
      },
      "source": [
        "We define a helper function to send prompts to Gemini and get the model's response. If there is a temporary error, it tries again a few times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-1cKZnYJCp7",
        "outputId": "59448c69-0d6a-4e3e-f954-4a04deadc533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini query function defined\n"
          ]
        }
      ],
      "source": [
        "# Function to query Gemini with a prompt, with simple retry logic\n",
        "def query_gemini(prompt, max_retries=5):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            time.sleep(1)  # Small pause between calls\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"Error attempt {attempt + 1}: {str(e)}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(5)  # Wait before retrying\n",
        "            else:\n",
        "                return \"ERROR\"\n",
        "\n",
        "print(\"Gemini query function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySf0r21HZZfW"
      },
      "source": [
        "We define two helper functions to parse the output from Gemini and convert it into a list of predicted NER labels for each word. The second function is a bit more robust and is used especially for method 4, where the model's output might be less regular.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epZiIRxMJJlD",
        "outputId": "6ca8e7f6-293c-423d-ffa0-02b9dacb0d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsing function(s) defined\n"
          ]
        }
      ],
      "source": [
        "# Simple parsing function for Gemini output (mainly for methods 1-3)\n",
        "def parse_response(response_text, original_sentence):\n",
        "    predicted_labels = ['O'] * len(original_sentence)\n",
        "\n",
        "    if response_text == \"ERROR\":\n",
        "        return predicted_labels\n",
        "\n",
        "    lines = response_text.strip().split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        matches = re.findall(r'(\\S+)/(PERSON|LOCATION|ORGANIZATION|MISCELLANEOUS|O)', line)\n",
        "        for word, label in matches:\n",
        "            word_clean = word.strip('.,!?\";:')\n",
        "            for i, orig_word in enumerate(original_sentence):\n",
        "                if orig_word.lower() == word_clean.lower():\n",
        "                    predicted_labels[i] = label\n",
        "                    break\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "# More robust parser, especially for method 4 (detailed definition)\n",
        "def parse_gemini_response(response_text, original_sentence):\n",
        "    predicted_labels = ['O'] * len(original_sentence)\n",
        "    lines = response_text.strip().split('\\n')\n",
        "    used_indices = set()\n",
        "    for line in lines:\n",
        "        matches = re.findall(r'(\\S+)/(PERSON|LOCATION|ORGANIZATION|MISCELLANEOUS|O)', line, re.IGNORECASE)\n",
        "        for word, label in matches:\n",
        "            word_clean = word.strip('.,!?\";:').lower()\n",
        "            for i, orig_word in enumerate(original_sentence):\n",
        "                if i not in used_indices and orig_word.lower() == word_clean:\n",
        "                    predicted_labels[i] = label.upper() if label != 'O' else 'O'\n",
        "                    used_indices.add(i)\n",
        "                    break\n",
        "    return predicted_labels\n",
        "\n",
        "print(\"Parsing function(s) defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-XY_jIZvYS"
      },
      "source": [
        "Since we adopted the IO tagging scheme for NER, we evaluate the model's performance by comparing the predicted labels with the gold labels on a token-by-token basis. For each token, we check whether the predicted entity type matches the true (gold) label. This approach allows us to compute standard metrics like precision, recall, and F1-score without requiring span-level matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YdmPp3IJXuG",
        "outputId": "3b493dc4-36eb-4e8e-8baf-e112866255d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation functions defined\n"
          ]
        }
      ],
      "source": [
        "# Compute precision, recall, F1 score for NER predictions\n",
        "def calculate_f1(true_labels_list, pred_labels_list):\n",
        "    correct = 0\n",
        "    total_true = 0\n",
        "    total_pred = 0\n",
        "\n",
        "    for true_labels, pred_labels in zip(true_labels_list, pred_labels_list):\n",
        "        for true_label, pred_label in zip(true_labels, pred_labels):\n",
        "\n",
        "            if true_label != 'O':\n",
        "                total_true += 1\n",
        "\n",
        "            if pred_label != 'O':\n",
        "                total_pred += 1\n",
        "\n",
        "            if true_label == pred_label and true_label != 'O':\n",
        "                correct += 1\n",
        "\n",
        "    precision = correct / total_pred if total_pred > 0 else 0\n",
        "    recall = correct / total_true if total_true > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'correct': correct,\n",
        "        'total_true': total_true,\n",
        "        'total_pred': total_pred\n",
        "    }\n",
        "\n",
        "print(\"Evaluation functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TXdmxDcZ_nQ"
      },
      "source": [
        "Here we test all four NER prompting methods on the first few examples from the development set. We print the original sentence, the true labels, and the predictions from each method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "lbX5i2fyJkhi",
        "outputId": "56b1aab6-425c-4a4b-9f44-991e48d34f48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " DEVELOPMENT PHASE - Testing on the first few examples\n",
            "\n",
            "--- Sentence 1 ---\n",
            "Text: CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY .\n",
            "True:  O O ORGANIZATION O O O O O O O O\n",
            "Met 1: O O ORGANIZATION O O O O O O O O\n",
            "Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "Met 2: O O ORGANIZATION O O O O O O O O\n",
            "Met 3: MISCELLANEOUS O ORGANIZATION O O O O O O O O\n",
            "Met 4: O O ORGANIZATION O O O O O O O O\n",
            "\n",
            "--- Sentence 2 ---\n",
            "Text: LONDON 1996-08-30\n",
            "True:  LOCATION O\n",
            "Met 1: LOCATION O\n",
            "Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Met 2: LOCATION O\n",
            "Met 3: LOCATION O\n",
            "Met 4: LOCATION O\n",
            "\n",
            "--- Sentence 3 ---\n",
            "Text: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\n",
            "True:  MISCELLANEOUS MISCELLANEOUS O PERSON PERSON O O O O O O O ORGANIZATION O ORGANIZATION O O O O O O O O O O O O O O O O O O O O\n",
            "Met 1: LOCATION LOCATION O PERSON PERSON O O O O O O O ORGANIZATION O ORGANIZATION O O O O O O O O O O O O O O O O O MISCELLANEOUS MISCELLANEOUS O\n",
            "Met 2: LOCATION LOCATION O PERSON PERSON O O O O O O O LOCATION O LOCATION O O O O O O O O O O O O O O O O O MISCELLANEOUS MISCELLANEOUS O\n",
            "Met 3: MISCELLANEOUS MISCELLANEOUS O PERSON PERSON O O O O O MISCELLANEOUS O LOCATION O LOCATION O O O O O O O O O O O O O O O O O MISCELLANEOUS MISCELLANEOUS O\n",
            "Met 4: MISCELLANEOUS MISCELLANEOUS O PERSON PERSON O O O O O O O ORGANIZATION O ORGANIZATION O O O O O O O O O O O O O O O O O MISCELLANEOUS MISCELLANEOUS O\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n DEVELOPMENT PHASE - Testing on the first few examples\")\n",
        "\n",
        "test_indices = [0, 1, 2]\n",
        "\n",
        "for i in test_indices:\n",
        "    sentence = dev_sentences[i]\n",
        "    true_labels = dev_labels_simple[i]\n",
        "    pos_tags = dev_pos[i]\n",
        "\n",
        "    print(f\"\\n--- Sentence {i+1} ---\")\n",
        "    print(f\"Text: {' '.join(sentence)}\")\n",
        "    print(f\"True:  {' '.join(true_labels)}\")\n",
        "\n",
        "    # Test Method 1\n",
        "    prompt1 = get_baseline_prompt(sentence)\n",
        "    response1 = query_gemini(prompt1)\n",
        "    pred1 = parse_response(response1, sentence)\n",
        "    print(f\"Met 1: {' '.join(pred1)}\")\n",
        "\n",
        "    # Test Method 2\n",
        "    prompt2 = get_decomposed_qa_prompt(sentence)\n",
        "    response2 = query_gemini(prompt2)\n",
        "    pred2 = parse_response(response2, sentence)\n",
        "    print(f\"Met 2: {' '.join(pred2)}\")\n",
        "\n",
        "    # Test Method 3\n",
        "    prompt3 = get_tool_augmentation_prompt(sentence, pos_tags)\n",
        "    response3 = query_gemini(prompt3)\n",
        "    pred3 = parse_response(response3, sentence)\n",
        "    print(f\"Met 3: {' '.join(pred3)}\")\n",
        "\n",
        "    # Test Method 4 (with detailed definitions)\n",
        "    prompt4 = get_detailed_prompt(sentence)\n",
        "    response4 = query_gemini(prompt4)\n",
        "    pred4 = parse_gemini_response(response4, sentence)\n",
        "    print(f\"Met 4: {' '.join(pred4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRro0J6ZauD1"
      },
      "source": [
        "Now we evaluate all four methods on a subset of 50 test sentences. For each method, we generate predictions, calculate metrics, and store the results for later comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TEdOp4NPBOHs",
        "outputId": "fcce7467-59b7-4bab-c704-f2c269ff0ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " FINAL EVALUATION ON THE TEST SET\n",
            "==================================================\n",
            "\n",
            " Testing Method 1 on 50 sentences...\n",
            "Sentence 1/50... ✓\n",
            "Sentence 2/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 3/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 4/50... ✓\n",
            "Sentence 5/50... ✓\n",
            "Sentence 6/50... ✓\n",
            "Sentence 7/50... ✓\n",
            "Sentence 8/50... ✓\n",
            "Sentence 9/50... ✓\n",
            "Sentence 10/50... ✓\n",
            "Sentence 11/50... ✓\n",
            "Sentence 12/50... ✓\n",
            "Sentence 13/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 14/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 15/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 16/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 17/50... ✓\n",
            "Sentence 18/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 19/50... Error attempt 1: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "Error attempt 2: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "Error attempt 3: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "✓\n",
            "Sentence 20/50... ✓\n",
            "Sentence 21/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 22/50... ✓\n",
            "Sentence 23/50... ✓\n",
            "Sentence 24/50... ✓\n",
            "Sentence 25/50... ✓\n",
            "Sentence 26/50... ✓\n",
            "Sentence 27/50... ✓\n",
            "Sentence 28/50... ✓\n",
            "Sentence 29/50... ✓\n",
            "Sentence 30/50... ✓\n",
            "Sentence 31/50... ✓\n",
            "Sentence 32/50... "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 279.23ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error attempt 1: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 380.87ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error attempt 2: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "✓\n",
            "Sentence 33/50... ✓\n",
            "Sentence 34/50... ✓\n",
            "Sentence 35/50... ✓\n",
            "Sentence 36/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 37/50... ✓\n",
            "Sentence 38/50... ✓\n",
            "Sentence 39/50... ✓\n",
            "Sentence 40/50... ✓\n",
            "Sentence 41/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 42/50... ✓\n",
            "Sentence 43/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 44/50... ✓\n",
            "Sentence 45/50... ✓\n",
            "Sentence 46/50... ✓\n",
            "Sentence 47/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 48/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 49/50... ✓\n",
            "Sentence 50/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "\n",
            " Testing Method 2 on 50 sentences...\n",
            "Sentence 1/50... ✓\n",
            "Sentence 2/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 3/50... ✓\n",
            "Sentence 4/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 5/50... ✓\n",
            "Sentence 6/50... ✓\n",
            "Sentence 7/50... ✓\n",
            "Sentence 8/50... ✓\n",
            "Sentence 9/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 10/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 11/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 12/50... ✓\n",
            "Sentence 13/50... ✓\n",
            "Sentence 14/50... ✓\n",
            "Sentence 15/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 16/50... ✓\n",
            "Sentence 17/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 18/50... ✓\n",
            "Sentence 19/50... ✓\n",
            "Sentence 20/50... ✓\n",
            "Sentence 21/50... ✓\n",
            "Sentence 22/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 23/50... ✓\n",
            "Sentence 24/50... ✓\n",
            "Sentence 25/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 26/50... ✓\n",
            "Sentence 27/50... ✓\n",
            "Sentence 28/50... ✓\n",
            "Sentence 29/50... ✓\n",
            "Sentence 30/50... ✓\n",
            "Sentence 31/50... ✓\n",
            "Sentence 32/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 33/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 34/50... ✓\n",
            "Sentence 35/50... ✓\n",
            "Sentence 36/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 37/50... ✓\n",
            "Sentence 38/50... ✓\n",
            "Sentence 39/50... ✓\n",
            "Sentence 40/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 41/50... ✓\n",
            "Sentence 42/50... ✓\n",
            "Sentence 43/50... ✓\n",
            "Sentence 44/50... ✓\n",
            "Sentence 45/50... ✓\n",
            "Sentence 46/50... ✓\n",
            "Sentence 47/50... ✓\n",
            "Sentence 48/50... ✓\n",
            "Sentence 49/50... ✓\n",
            "Sentence 50/50... ✓\n",
            "\n",
            " Testing Method 3 on 50 sentences...\n",
            "Sentence 1/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 2/50... ✓\n",
            "Sentence 3/50... ✓\n",
            "Sentence 4/50... ✓\n",
            "Sentence 5/50... ✓\n",
            "Sentence 6/50... ✓\n",
            "Sentence 7/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 8/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 9/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 10/50... ✓\n",
            "Sentence 11/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 12/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 13/50... ✓\n",
            "Sentence 14/50... ✓\n",
            "Sentence 15/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 16/50... ✓\n",
            "Sentence 17/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 18/50... ✓\n",
            "Sentence 19/50... ✓\n",
            "Sentence 20/50... ✓\n",
            "Sentence 21/50... ✓\n",
            "Sentence 22/50... ✓\n",
            "Sentence 23/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 24/50... ✓\n",
            "Sentence 25/50... ✓\n",
            "Sentence 26/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 27/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 28/50... ✓\n",
            "Sentence 29/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 30/50... ✓\n",
            "Sentence 31/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 32/50... ✓\n",
            "Sentence 33/50... ✓\n",
            "Sentence 34/50... ✓\n",
            "Sentence 35/50... ✓\n",
            "Sentence 36/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 37/50... ✓\n",
            "Sentence 38/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 39/50... ✓\n",
            "Sentence 40/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 41/50... ✓\n",
            "Sentence 42/50... ✓\n",
            "Sentence 43/50... ✓\n",
            "Sentence 44/50... ✓\n",
            "Sentence 45/50... ✓\n",
            "Sentence 46/50... ✓\n",
            "Sentence 47/50... ✓\n",
            "Sentence 48/50... ✓\n",
            "Sentence 49/50... ✓\n",
            "Sentence 50/50... ✓\n",
            "\n",
            " Testing Method 4 on 50 sentences...\n",
            "Sentence 1/50... ✓\n",
            "Sentence 2/50... ✓\n",
            "Sentence 3/50... ✓\n",
            "Sentence 4/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 5/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 6/50... ✓\n",
            "Sentence 7/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 8/50... ✓\n",
            "Sentence 9/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 10/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 11/50... ✓\n",
            "Sentence 12/50... ✓\n",
            "Sentence 13/50... ✓\n",
            "Sentence 14/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 15/50... ✓\n",
            "Sentence 16/50... ✓\n",
            "Sentence 17/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 18/50... ✓\n",
            "Sentence 19/50... Error attempt 1: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 1.\n",
            "✓\n",
            "Sentence 20/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 21/50... ✓\n",
            "Sentence 22/50... ✓\n",
            "Sentence 23/50... ✓\n",
            "Sentence 24/50... ✓\n",
            "Sentence 25/50... ✓\n",
            "Sentence 26/50... ✓\n",
            "Sentence 27/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 28/50... ✓\n",
            "Sentence 29/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 30/50... ✓\n",
            "Sentence 31/50... ✓\n",
            "Sentence 32/50... ✓\n",
            "Sentence 33/50... ✓\n",
            "Sentence 34/50... ✓\n",
            "Sentence 35/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 36/50... ✓\n",
            "Sentence 37/50... ✓\n",
            "Sentence 38/50... ✓\n",
            "Sentence 39/50... ✓\n",
            "Sentence 40/50... ✓\n",
            "Sentence 41/50... ✓\n",
            "Sentence 42/50... ✓\n",
            "Sentence 43/50... ✓\n",
            "Sentence 44/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 45/50... ✓\n",
            "Sentence 46/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n",
            "Sentence 47/50... Error attempt 1: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "✓\n",
            "Sentence 48/50... ✓\n",
            "Sentence 49/50... ✓\n",
            "Sentence 50/50... Error attempt 1: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "✓\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n FINAL EVALUATION ON THE TEST SET\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "n_test = 50  # Number of test sentences to use\n",
        "test_subset_sentences = test_sentences[:n_test]\n",
        "test_subset_labels = test_labels_simple[:n_test]\n",
        "test_subset_pos = test_pos[:n_test]\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "# METHOD 1: Baseline\n",
        "print(f\"\\n Testing Method 1 on {n_test} sentences...\")\n",
        "method1_predictions = []\n",
        "\n",
        "for i, sentence in enumerate(test_subset_sentences):\n",
        "    print(f\"Sentence {i+1}/{n_test}...\", end=\" \")\n",
        "    prompt = get_baseline_prompt(sentence)\n",
        "    response = query_gemini(prompt)\n",
        "    predictions = parse_response(response, sentence)\n",
        "    method1_predictions.append(predictions)\n",
        "    print(\"✓\")\n",
        "\n",
        "results['Method 1'] = calculate_f1(test_subset_labels, method1_predictions)\n",
        "\n",
        "\n",
        "\n",
        "# METHOD 2:\n",
        "print(f\"\\n Testing Method 2 on {n_test} sentences...\")\n",
        "method2_predictions = []\n",
        "\n",
        "for i, sentence in enumerate(test_subset_sentences):\n",
        "    print(f\"Sentence {i+1}/{n_test}...\", end=\" \")\n",
        "    prompt = get_decomposed_qa_prompt(sentence)\n",
        "    response = query_gemini(prompt)\n",
        "    predictions = parse_response(response, sentence)\n",
        "    method2_predictions.append(predictions)\n",
        "    print(\"✓\")\n",
        "\n",
        "results['Method 2'] = calculate_f1(test_subset_labels, method2_predictions)\n",
        "\n",
        "\n",
        "\n",
        "# METHOD 3:\n",
        "print(f\"\\n Testing Method 3 on {n_test} sentences...\")\n",
        "method3_predictions = []\n",
        "\n",
        "for i, sentence in enumerate(test_subset_sentences):\n",
        "    print(f\"Sentence {i+1}/{n_test}...\", end=\" \")\n",
        "    prompt = get_tool_augmentation_prompt(sentence, test_subset_pos)\n",
        "    response = query_gemini(prompt)\n",
        "    predictions = parse_response(response, sentence)\n",
        "    method3_predictions.append(predictions)\n",
        "    print(\"✓\")\n",
        "\n",
        "results['Method 3'] = calculate_f1(test_subset_labels, method3_predictions)\n",
        "\n",
        "\n",
        "\n",
        "# METHOD 4:\n",
        "print(f\"\\n Testing Method 4 on {n_test} sentences...\")\n",
        "method4_predictions = []\n",
        "\n",
        "for i, sentence in enumerate(test_subset_sentences):\n",
        "    print(f\"Sentence {i+1}/{n_test}...\", end=\" \")\n",
        "    prompt = get_detailed_prompt(sentence)\n",
        "    response = query_gemini(prompt)\n",
        "    predictions = parse_gemini_response(response, sentence)\n",
        "    method4_predictions.append(predictions)\n",
        "    print(\"✓\")\n",
        "\n",
        "results['Method 4'] = calculate_f1(test_subset_labels, method4_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL59XI8WlrZZ"
      },
      "source": [
        "We print a summary table of the final results (precision, recall, F1) for each method. We also show more detailed counts and highlight the best-performing method based on F1 score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8C3ZlG4Pzl2",
        "outputId": "2677b3e7-6637-483a-dd72-868fab158609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL RESULTS\n",
            "============================================================\n",
            "\n",
            "Method                    Precision  Recall     F1-Score  \n",
            "-------------------------------------------------------\n",
            "Method 1                  0.942      0.913      0.927\n",
            "Method 2                  0.901      0.905      0.903\n",
            "Method 3                  0.917      0.909      0.913\n",
            "Method 4                  0.953      0.974      0.964\n",
            "\n",
            " DETAILS:\n",
            "\n",
            " Method 1:\n",
            "   Precision: 0.942\n",
            "   Recall: 0.913\n",
            "   F1-Score: 0.927\n",
            "   Correct entities: 211\n",
            "   True entities: 231\n",
            "   Predicted entities: 224\n",
            "\n",
            " Method 2:\n",
            "   Precision: 0.901\n",
            "   Recall: 0.905\n",
            "   F1-Score: 0.903\n",
            "   Correct entities: 209\n",
            "   True entities: 231\n",
            "   Predicted entities: 232\n",
            "\n",
            " Method 3:\n",
            "   Precision: 0.917\n",
            "   Recall: 0.909\n",
            "   F1-Score: 0.913\n",
            "   Correct entities: 210\n",
            "   True entities: 231\n",
            "   Predicted entities: 229\n",
            "\n",
            " Method 4:\n",
            "   Precision: 0.953\n",
            "   Recall: 0.974\n",
            "   F1-Score: 0.964\n",
            "   Correct entities: 225\n",
            "   True entities: 231\n",
            "   Predicted entities: 236\n",
            "\n",
            "🏆 BEST METHOD: Method 4 (F1: 0.964)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print results table\n",
        "print(f\"\\n{'Method':<25} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "for method_name, metrics in results.items():\n",
        "    print(f\"{method_name:<25} {metrics['precision']:.3f}      {metrics['recall']:.3f}      {metrics['f1']:.3f}\")\n",
        "\n",
        "# Show details for each method\n",
        "print(f\"\\n DETAILS:\")\n",
        "for method_name, metrics in results.items():\n",
        "    print(f\"\\n {method_name}:\")\n",
        "    print(f\"   Precision: {metrics['precision']:.3f}\")\n",
        "    print(f\"   Recall: {metrics['recall']:.3f}\")\n",
        "    print(f\"   F1-Score: {metrics['f1']:.3f}\")\n",
        "    print(f\"   Correct entities: {metrics['correct']}\")\n",
        "    print(f\"   True entities: {metrics['total_true']}\")\n",
        "    print(f\"   Predicted entities: {metrics['total_pred']}\")\n",
        "\n",
        "# Print the best method (highest F1)\n",
        "best_method = max(results.items(), key=lambda x: x[1]['f1'])\n",
        "print(f\"\\n🏆 BEST METHOD: {best_method[0]} (F1: {best_method[1]['f1']:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMrGQwShl4Zi"
      },
      "source": [
        "Finally, we show a few examples from the test set for a qualitative analysis. For each, we print the sentence, true labels, and predictions from the first three methods to better see where the models perform well or make mistakes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSqfNk_BTZq9",
        "outputId": "8c013c34-5e36-4360-a431-52bd7f84b5c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "QUALITATIVE ANALYSIS - Examples\n",
            "============================================================\n",
            "\n",
            "--- Example 34 ---\n",
            "Sentence: Squad : Javier Pertile , Paolo Vaccari , Marcello Cuttitta , Ivan Francescato , Leandro Manteri , Diego Dominguez , Francesco Mazzariol , Alessandro Troncon , Orazio Arancio , Andrea Sgorlon , Massimo Giovanelli , Carlo Checchinato , Walter Cristofoletto , Franco Properzi Curti , Carlo Orlandi , Massimo Cuttitta , Giambatista Croci , Gianluca Guidi , Nicola Mazzucato , Alessandro Moscardi , Andrea Castellani .\n",
            "True:     O O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O\n",
            "Method 1: O O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON PERSON O O PERSON O O O O PERSON PERSON O PERSON PERSON O PERSON PERSON O O PERSON O O PERSON O\n",
            "Method 2: O O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON PERSON O O PERSON O O O O PERSON PERSON O PERSON PERSON O PERSON PERSON O O PERSON O O PERSON O\n",
            "Method 3: O O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON PERSON O O PERSON O O O O PERSON PERSON O PERSON PERSON O PERSON PERSON O O PERSON O O PERSON O\n",
            "Method 4: O O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O PERSON PERSON O\n",
            "\n",
            "--- Example 27 ---\n",
            "Sentence: Cuttitta , who trainer George Coste said was certain to play on Saturday week , was named in a 21-man squad lacking only two of the team beaten 54-21 by England at Twickenham last month .\n",
            "True:     PERSON O O O PERSON PERSON O O O O O O O O O O O O O O O O O O O O O O O O LOCATION O LOCATION O O O\n",
            "Method 1: PERSON O O O PERSON PERSON O O O O O O O O O O O O O O O O O O O O O O O O ORGANIZATION O LOCATION O O O\n",
            "Method 2: PERSON O O O PERSON PERSON O O O O O O O O O O O O O O O O O O O O O O O O LOCATION O LOCATION O O O\n",
            "Method 3: PERSON O O O PERSON PERSON O O O O O O O O O O O O O O O O O O O O O O O O LOCATION O LOCATION O O O\n",
            "Method 4: PERSON O O O PERSON PERSON O O O O O O O O O O O O O O O O O O O O O O O O LOCATION O LOCATION O O O\n",
            "\n",
            "--- Example 39 ---\n",
            "Sentence: It was the second Syrian defensive blunder in four minutes .\n",
            "True:     O O O O MISCELLANEOUS O O O O O O\n",
            "Method 1: O O O O O O O O O O O\n",
            "Method 2: O O O O LOCATION O O O O O O\n",
            "Method 3: O O O O LOCATION O O O O O O\n",
            "Method 4: O O O O MISCELLANEOUS O O O O O O\n",
            "\n",
            "--- Example 49 ---\n",
            "Sentence: FREESTYLE SKIING-WORLD CUP MOGUL RESULTS .\n",
            "True:     O MISCELLANEOUS MISCELLANEOUS O O O\n",
            "Method 1: MISCELLANEOUS MISCELLANEOUS MISCELLANEOUS MISCELLANEOUS O O\n",
            "Method 2: MISCELLANEOUS O MISCELLANEOUS MISCELLANEOUS O O\n",
            "Method 3: MISCELLANEOUS MISCELLANEOUS MISCELLANEOUS MISCELLANEOUS O O\n",
            "Method 4: MISCELLANEOUS MISCELLANEOUS MISCELLANEOUS MISCELLANEOUS O O\n",
            "\n",
            "--- Example 26 ---\n",
            "Sentence: on Friday for their friendly against Scotland at Murrayfield more than a year after the 30-year-old wing announced he was retiring following differences over selection .\n",
            "True:     O O O O O O LOCATION O LOCATION O O O O O O O O O O O O O O O O O\n",
            "Method 1: O O O O O O LOCATION O LOCATION O O O O O O O O O O O O O O O O O\n",
            "Method 2: O O O O O O LOCATION O LOCATION O O O O O O O O O O O O O O O O O\n",
            "Method 3: O O O O O O LOCATION O LOCATION O O O O O O O O O O O O O O O O O\n",
            "Method 4: O O O O O O LOCATION O LOCATION O O O O O O O O O O O O O O O O O\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 60)\n",
        "print(\"QUALITATIVE ANALYSIS - Examples\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i in range(min(5, len(test_subset_sentences))):\n",
        "    i = np.random.randint(0, len(test_subset_sentences))\n",
        "    sentence = test_subset_sentences[i]\n",
        "    true_labels = test_subset_labels[i]\n",
        "\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Sentence: {' '.join(sentence)}\")\n",
        "    print(f\"True:     {' '.join(true_labels)}\")\n",
        "    print(f\"Method 1: {' '.join(method1_predictions[i])}\")\n",
        "    print(f\"Method 2: {' '.join(method2_predictions[i])}\")\n",
        "    print(f\"Method 3: {' '.join(method3_predictions[i])}\")\n",
        "    print(f\"Method 4: {' '.join(method4_predictions[i])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNIOy6uWmNsU"
      },
      "source": [
        "## Final Discussion\n",
        "\n",
        "The results show that all four zero-shot NER methods perform surprisingly well using Gemini.\n",
        "\n",
        "Method 1 (Baseline) performed very well (F1: 0.927) considering its simplicity, demonstrating that Gemini 2.5 has strong inherent NER capabilities even with minimal prompting. However, it showed the lowest recall (0.913), suggesting it tends to be more conservative in entity identification.\n",
        "\n",
        "Methods 2 and 3 (Decomposed QA and POS Augmentation) showed similar performance (F1: 0.903 and 0.913 respectively), falling between the baseline and detailed approaches. It's surprising how little difference there is between those two methods, considering that Method 3 also includes POS tags. This might be because modern LLMs already have a strong internal representation of syntax, making explicit POS information less impactful.\n",
        "It's also surprising that they perform worse than the baseline, despite the additional information. Probably asking the LLM to generate all the labels in a single prompt, instead of making separate requests for each one, leads the model to approach the task globally rather than focusing on one label at a time. As a result, the advantage of using separate requests, where the model can dedicate more attention and context to each individual label, is lost.\n",
        "\n",
        "Method 4 (Detailed Definitions) emerged as the clear winner with an F1 score of 0.964, achieving both the highest precision (0.953) and recall (0.974). Providing clear descriptions of each entity type reduces ambiguity and help capture edge cases that might be missed by simpler prompts.\n",
        "\n",
        "Overall, zero-shot LLMs are competitive on standard NER with careful prompt engineering. However, the output format is not always perfectly consistent, and errors tend to happen in more ambiguous cases (like event names or less frequent classes). Using a more robust parsing strategy and experimenting with even more detailed prompts could further improve performance.\n",
        "\n",
        "In the future, it could be useful to compare the zero-shot results from Gemini 2.5 with results from supervised models that are fine-tuned on the same dataset. This would help us understand the differences between using a powerful general model and a model trained specifically for the task.\n",
        "\n",
        "Another idea is to test the same prompts on domain-specific NER datasets, such as biomedical or legal texts. These domains use different types of entities and vocabulary, so it would be interesting to see how well the model can adapt.\n",
        "\n",
        "Finally, it would be interesting to repeat the experiment using a smaller and less powerful language model. In that case, the way the prompt is written might have a bigger impact on the results, because simpler models may need more help to understand the task.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
